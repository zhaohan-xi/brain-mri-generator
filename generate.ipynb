{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5766ef0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brain-mri/miniconda3/envs/py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/brain-mri/miniconda3/envs/py311/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/brain-mri/miniconda3/envs/py311/lib/python3.11/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/tmp/ipykernel_1596855/3340844866.py:161: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coord_hr = torch.tensor(coord_hr).cuda().float()\n",
      "441it [06:44,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "import torch\n",
    "import SimpleITK as sitk\n",
    "import glob\n",
    "import utils\n",
    "from utils_clip.simple_tokenizer import SimpleTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "from itertools import product\n",
    "from CLIP.model import CLIP\n",
    "from utils_clip import load_config_file\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# checkpoint_path = 'checkpoint_CLIP.pt'\n",
    "MODEL_CONFIG_PATH = 'CLIP/model_config.yaml'\n",
    "model_config = load_config_file(MODEL_CONFIG_PATH)\n",
    "\n",
    "tokenizer = SimpleTokenizer()\n",
    "model_params = dict(model_config.RN50)\n",
    "model_params['vision_layers'] = tuple(model_params['vision_layers'])\n",
    "model_params['vision_patch_size'] = None\n",
    "model = CLIP(**model_params)\n",
    "# checkpoint = torch.load(checkpoint_path)\n",
    "# state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# model.load_state_dict(state_dict)\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def tokenize(texts, tokenizer, context_length=90):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "    return result\n",
    "\n",
    "def img_pad(img, target_shape):\n",
    "    current_shape = img.shape\n",
    "    pads = [(0, max(0, target_shape[i] - current_shape[i])) for i in range(len(target_shape))]\n",
    "    padded_img = np.pad(img, pads, mode='constant', constant_values=0)\n",
    "    current_shape_2 = padded_img.shape\n",
    "    crops = []\n",
    "    for i in range(len(target_shape)):\n",
    "        if current_shape_2[i] > target_shape[i]:\n",
    "            crops.append(\n",
    "                slice((current_shape_2[i] - target_shape[i]) // 2, (current_shape_2[i] + target_shape[i]) // 2))\n",
    "        else:\n",
    "            crops.append(slice(None))\n",
    "    cropped_img = padded_img[tuple(crops)]\n",
    "    return cropped_img\n",
    "\n",
    "def calculate_patch_index(target_size, patch_size, overlap_ratio=0.25):\n",
    "    shape = target_size\n",
    "\n",
    "    gap = int(patch_size[0] * (1 - overlap_ratio))\n",
    "    index1 = [f for f in range(shape[0])]\n",
    "    index_x = index1[::gap]\n",
    "    index2 = [f for f in range(shape[1])]\n",
    "    index_y = index2[::gap]\n",
    "    index3 = [f for f in range(shape[2])]\n",
    "    index_z = index3[::gap]\n",
    "\n",
    "    index_x = [f for f in index_x if f < shape[0] - patch_size[0]]\n",
    "    index_x.append(shape[0] - patch_size[0])\n",
    "    index_y = [f for f in index_y if f < shape[1] - patch_size[1]]\n",
    "    index_y.append(shape[1] - patch_size[1])\n",
    "    index_z = [f for f in index_z if f < shape[2] - patch_size[2]]\n",
    "    index_z.append(shape[2] - patch_size[2])\n",
    "\n",
    "    start_pos = list()\n",
    "    loop_val = [index_x, index_y, index_z]\n",
    "    for i in product(*loop_val):\n",
    "        start_pos.append(i)\n",
    "    return start_pos\n",
    "\n",
    "def patch_slicer(img_vol_0, overlap_ratio, crop_size, scale0, scale1, scale2):\n",
    "    W, H, D = img_vol_0.shape\n",
    "    pos = calculate_patch_index((W, H, D), crop_size, overlap_ratio)\n",
    "    scan_patches = []\n",
    "    patch_idx = []\n",
    "    for start_pos in pos:\n",
    "        img_0_lr_patch = img_vol_0[start_pos[0]:start_pos[0] + crop_size[0], start_pos[1]:start_pos[1] + crop_size[1],\n",
    "                         start_pos[2]:start_pos[2] + crop_size[2]]\n",
    "        #print(img_0_lr_patch.shape)\n",
    "        scan_patches.append(torch.tensor(img_0_lr_patch).float().unsqueeze(0))\n",
    "        patch_idx.append([int(start_pos[0]), int(start_pos[0])+int(crop_size[0] * scale0), int(start_pos[1]), int(start_pos[1])+int(crop_size[1] * scale1), int(start_pos[2]), int(start_pos[2])+int(crop_size[2] * scale2)])\n",
    "    return scan_patches, patch_idx\n",
    "\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, patches):\n",
    "        self.patches = patches\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.patches[idx]\n",
    "\n",
    "def _get_pred(model, dataloader, coord_hr, seq_tgt, crop_size, out_dir):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm(enumerate(dataloader)):\n",
    "            batchsize = batch.size(0)  # batch size (60, 60, 60)\n",
    "            input_patch = batch.cuda()\n",
    "            batch_coord_hr = coord_hr.repeat(batchsize, 1, 1)\n",
    "            tgt_prompt = seq_tgt.repeat(batchsize, 1)\n",
    "            pred_0_1_patch = model.generation(input_patch, batch_coord_hr, tgt_prompt.cuda().float()) # (1, crop_size*3, 1)\n",
    "            pred_0_1_patch = pred_0_1_patch.reshape(crop_size) # (crop_size, crop_size, crop_size)\n",
    "            # utils.write_img(\n",
    "            #     pred_0_1_patch.reshape(crop_size), \n",
    "            #     os.path.join(f'{out_dir}/generated_{i}.nii.gz'),\n",
    "            #     os.path.join(img_path_1, 'test_HCPD_T2w.nii.gz'), \n",
    "            #     new_spacing=None)\n",
    "            results.append(pred_0_1_patch)\n",
    "            # print(torch.stack(results, dim=0).shape)\n",
    "    return results   # (1, crop_size, crop_size, crop_size), list of tensor.cuda()\n",
    "\n",
    "# Above are input pre-processing (divide the original 3D into patches)\n",
    "#####################################################################\n",
    "# Below are integrate generated patches into a unified 3D image\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "psnr_0_1_list = []\n",
    "psnr_1_0_list = []\n",
    "ssim_0_1_list = []\n",
    "ssim_1_0_list = []\n",
    "# model_pth = './save/checkpoint.pth'\n",
    "model_pth = './checkpoint.pth'\n",
    "model_img = models.make(torch.load(model_pth)['model_G'], load_sd=True).cuda()\n",
    "img_path_0 = './Experimental_data/image'\n",
    "img_path_1 = './Experimental_data/image' # Using to provide target image spacing, it is not necessary, the target image spacing can be manually set\n",
    "img_list_0 = sorted(os.listdir(img_path_0))\n",
    "img_list_1 = sorted(os.listdir(img_path_1))\n",
    "prompt_M1 = './Experimental_data/test_HCPD_T2w_prompt.txt'\n",
    "with open(prompt_M1) as f1:\n",
    "    lines_M1 = f1.readlines()\n",
    "\n",
    "img_0 = sitk.ReadImage(os.path.join(img_path_0, 'test_HCPD_T1w.nii.gz'))\n",
    "img_0_spacing = img_0.GetSpacing()\n",
    "img_vol_0 = sitk.GetArrayFromImage(img_0)\n",
    "H, W, D = img_vol_0.shape\n",
    "img_vol_0 = img_pad(img_vol_0, target_shape=(H, W, D))\n",
    "img_vol_0 = utils.percentile_clip(img_vol_0)\n",
    "coord_size = [60, 60, 60]\n",
    "coord_hr = utils.make_coord(coord_size, flatten=True)\n",
    "coord_hr = torch.tensor(coord_hr).cuda().float()\n",
    "text_tgt = lines_M1[0].replace('\"', '')\n",
    "text_tgt = text_tgt.strip((text_tgt.strip().split(':'))[0])\n",
    "text_tgt = text_tgt.strip(text_tgt[0])\n",
    "seq_tgt = tokenize(text_tgt, tokenizer).cuda()\n",
    "with torch.no_grad():\n",
    "    seq_tgt = model.encode_text(seq_tgt)\n",
    "crop_size = (60, 60, 60)\n",
    "scale0 = coord_size[0] / crop_size[0]\n",
    "scale1 = coord_size[1] / crop_size[1]\n",
    "scale2 = coord_size[2] / crop_size[2]\n",
    "patches, _ = patch_slicer(img_vol_0, 0.5, crop_size, scale0, scale1, scale2)\n",
    "dataset = PatchDataset(patches)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "\n",
    "out_dir = \"./save_img/nii/\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "pred_0_1 = _get_pred(model_img, dataloader, coord_hr, seq_tgt, crop_size, out_dir)\n",
    "pred_0_1 = [x.cpu().numpy() for x in pred_0_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c6c4ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stitching 441 patches into output shape (227, 272, 227)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [00:02<00:00, 213.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved stitched volume to ./save_img/nii/stitched_volume.nii.gz\n"
     ]
    }
   ],
   "source": [
    "def create_weight_mask(patch_shape, overlap_ratio=0.5, sigma_factor=0.15):\n",
    "    \"\"\"\n",
    "    Create a weight mask for blending patches smoothly.\n",
    "    Uses a 3D Gaussian-based weighting to prioritize central voxels over edge voxels.\n",
    "    \n",
    "    Args:\n",
    "        patch_shape: Tuple of (depth, height, width) for the patch\n",
    "        overlap_ratio: Amount of overlap between patches\n",
    "        sigma_factor: Controls the steepness of the weight falloff\n",
    "        \n",
    "    Returns:\n",
    "        3D numpy array of same shape as patch with weights between 0 and 1\n",
    "    \"\"\"\n",
    "    # Create coordinates relative to center\n",
    "    z, y, x = np.indices(patch_shape)\n",
    "    center = np.array([(d - 1) / 2 for d in patch_shape])\n",
    "    \n",
    "    # Calculate distance from center (normalized)\n",
    "    z_norm = (z - center[0]) / (patch_shape[0]/2)\n",
    "    y_norm = (y - center[1]) / (patch_shape[1]/2)\n",
    "    x_norm = (x - center[2]) / (patch_shape[2]/2)\n",
    "    \n",
    "    # Combine for radial distance\n",
    "    sigma = sigma_factor * (1 + 1/overlap_ratio)\n",
    "    distance = np.sqrt(z_norm**2 + y_norm**2 + x_norm**2)\n",
    "    \n",
    "    # Convert to weights with Gaussian falloff\n",
    "    weights = np.exp(-(distance**2) / (2 * sigma**2))\n",
    "    \n",
    "    return weights\n",
    "\n",
    "\n",
    "def stitch_patches(patch_vol: list, patch_indices, output_shape, overlap_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Stitch together a set of overlapping patches into a single volume.\n",
    "    \n",
    "    Args:\n",
    "        patch_vol: List of np.ndarray\n",
    "        patch_indices: List of [z_start, z_end, y_start, y_end, x_start, x_end] for each patch\n",
    "        output_shape: Tuple of (depth, height, width) for the final volume\n",
    "        overlap_ratio: Amount of overlap between adjacent patches\n",
    "        \n",
    "    Returns:\n",
    "        Stitched volume as a numpy array\n",
    "    \"\"\"\n",
    "    # Initialize output volume and weight accumulator\n",
    "    output_volume = np.zeros(output_shape, dtype=np.float32)\n",
    "    weight_accumulator = np.zeros(output_shape, dtype=np.float32)\n",
    "    \n",
    "    print(f\"Stitching {len(patch_vol)} patches into output shape {output_shape}...\")\n",
    "    \n",
    "    for i, (patch_data, patch_idx) in enumerate(tqdm(zip(patch_vol, patch_indices), total=len(patch_vol))):\n",
    "        \n",
    "        # Get patch coordinates\n",
    "        z_start, z_end, y_start, y_end, x_start, x_end = patch_idx\n",
    "        \n",
    "        # Ensure patches don't exceed output volume dimensions\n",
    "        z_end = min(z_end, output_shape[0])\n",
    "        y_end = min(y_end, output_shape[1])\n",
    "        x_end = min(x_end, output_shape[2])\n",
    "        \n",
    "        # Get patch shape\n",
    "        patch_shape = (z_end - z_start, y_end - y_start, x_end - x_start)\n",
    "        \n",
    "        # Check if loaded patch needs reshaping\n",
    "        if patch_data.shape != patch_shape:\n",
    "            # Reshape patch if needed\n",
    "            print(f\"Reshape patch if needed {patch_data.shape}->{patch_shape}\")\n",
    "            patch_data = patch_data[:patch_shape[0], :patch_shape[1], :patch_shape[2]]\n",
    "            \n",
    "        # Create weight mask for this patch\n",
    "        weights = create_weight_mask(patch_shape, overlap_ratio)\n",
    "        \n",
    "        # Add weighted patch to output\n",
    "        output_volume[z_start:z_end, y_start:y_end, x_start:x_end] += patch_data * weights\n",
    "        weight_accumulator[z_start:z_end, y_start:y_end, x_start:x_end] += weights\n",
    "        \n",
    "    # Avoid division by zero\n",
    "    mask = weight_accumulator > 0\n",
    "    output_volume[mask] = output_volume[mask] / weight_accumulator[mask]\n",
    "    \n",
    "    return output_volume\n",
    "\n",
    "\n",
    "def save_stitched_volume(output_volume, reference_file, output_path):\n",
    "    \"\"\"\n",
    "    Save the stitched volume as a NIfTI file, copying metadata from a reference image.\n",
    "    \n",
    "    Args:\n",
    "        output_volume: The stitched volume as numpy array\n",
    "        reference_file: Path to a reference NIfTI file to copy metadata from\n",
    "        output_path: Path where the stitched volume will be saved\n",
    "    \"\"\"\n",
    "    # Read reference image to get metadata\n",
    "    reference_img = sitk.ReadImage(reference_file)\n",
    "    \n",
    "    # Convert the stitched volume to an ITK image\n",
    "    output_img = sitk.GetImageFromArray(output_volume)\n",
    "    \n",
    "    # Copy metadata from reference image\n",
    "    output_img.SetSpacing(reference_img.GetSpacing())\n",
    "    output_img.SetOrigin(reference_img.GetOrigin())\n",
    "    output_img.SetDirection(reference_img.GetDirection())\n",
    "    \n",
    "    # Save the image\n",
    "    sitk.WriteImage(output_img, output_path)\n",
    "    print(f\"Saved stitched volume to {output_path}\")\n",
    "\n",
    "\n",
    "def stitch(patch_vol: list, original_img_path=None):\n",
    "\n",
    "    # Load the first patch to get dimensions\n",
    "    first_patch = patch_vol[0]\n",
    "    patch_size = first_patch.shape\n",
    "    \n",
    "    if original_img_path:\n",
    "        original_img = sitk.ReadImage(original_img_path)\n",
    "        output_shape = sitk.GetArrayFromImage(original_img).shape\n",
    "    else:\n",
    "        # If original image is not available, you need to load or reconstruct patch indices\n",
    "        # For this example, we'll assume the output shape is known or can be determined\n",
    "        # from your patch generation logic\n",
    "        # Placeholder example:\n",
    "        output_shape = (256, 256, 256)  # Replace with actual dimensions\n",
    "\n",
    "    # Load or reconstruct patch indices\n",
    "    # For this example, we'll calculate it from scratch using the pattern from the original code\n",
    "    crop_size = patch_size\n",
    "    overlap_ratio = 0.5\n",
    "    \n",
    "    # Calculate patch indices\n",
    "    patch_indices = []\n",
    "    patch_start_positions = calculate_patch_index(output_shape, crop_size, overlap_ratio)\n",
    "    \n",
    "    for pos in patch_start_positions:\n",
    "        z_start, y_start, x_start = pos\n",
    "        z_end = min(z_start + crop_size[0], output_shape[0])\n",
    "        y_end = min(y_start + crop_size[1], output_shape[1])\n",
    "        x_end = min(x_start + crop_size[2], output_shape[2])\n",
    "        patch_indices.append([z_start, z_end, y_start, y_end, x_start, x_end])\n",
    "    \n",
    "    # Check if we have enough patches\n",
    "    if len(patch_indices) != len(patch_vol):\n",
    "        print(f\"Warning: Number of patch files ({len(patch_vol)}) does not match calculated indices ({len(patch_indices)})\")\n",
    "        print(\"Using the minimum of the two\")\n",
    "        patch_vol = patch_vol[:min(len(patch_vol), len(patch_indices))]\n",
    "        patch_indices = patch_indices[:min(len(patch_vol), len(patch_indices))]\n",
    "    \n",
    "    # Stitch patches together\n",
    "    stitched_volume = stitch_patches(patch_vol, patch_indices, output_shape, overlap_ratio)\n",
    "    return stitched_volume\n",
    "   \n",
    "reference_file = os.path.join(img_path_1, 'test_HCPD_T2w.nii.gz')\n",
    "stitched_volume = stitch(pred_0_1, reference_file)\n",
    "    # Save the stitched volume\n",
    "    \n",
    "output_path = os.path.join(out_dir, 'stitched_volume.nii.gz')\n",
    "save_stitched_volume(stitched_volume, reference_file, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a559e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
